### Rupture — Droplet Hi-C Processing Pipeline
### Author: Yang Xie
###
### Usage (via the rupture CLI):
###   rupture run --configfile config.yaml --cores 16
###   rupture run --configfile config.yaml --dryrun
###
### Required directory layout in working directory:
###   01.rawdata/{sample}_R1.fq.gz
###   01.rawdata/{sample}_R2.fq.gz
###   01.rawdata/{sample}_R3.fq.gz
###   config.yaml

import os
import math
import re as _re
import glob as _glob

# ── Configuration ──
bin_size = int(config["bin_size"])

# Required reference paths — no defaults, must be set in config
bc_ref = config["bc_ref"]
ref = config["ref"]
chrom_size = config["chrom_size"]

# ── Chunked processing ──
chunk_size_gb = float(config.get("chunk_size", 0))

def _get_chunk_ids(sample):
    """Compute chunk IDs for a sample based on R1 FASTQ size."""
    r1 = f"01.rawdata/{sample}_R1.fq.gz"
    if chunk_size_gb <= 0 or not os.path.isfile(r1):
        return ["part_001"]
    size_gb = os.path.getsize(r1) / (1024 ** 3)
    n = max(1, math.ceil(size_gb / chunk_size_gb))
    return [f"part_{i:03d}" for i in range(1, n + 1)]

# Pre-compute chunk IDs for all samples
CHUNKS = {s: _get_chunk_ids(s) for s in config["sample_id"]}

# ── Helper: runtime check for previous mapping BAMs ──
def get_merge_inputs(wildcards):
    """If a previous BAM exists in 07.prev_mapping/, include it for merging."""
    new_bam = f"03.mapping/{wildcards.sample}_{wildcards.genome}.bam"
    old_bam = f"07.prev_mapping/{wildcards.sample}_{wildcards.genome}.bam"
    if os.path.isfile(old_bam):
        return [new_bam, old_bam]
    return [new_bam]

# ── Helper: collect per-chunk outputs via checkpoint ──
def _get_chunks_for_sample(wildcards):
    """Resolve chunk IDs after the split_fastq checkpoint completes."""
    chk = checkpoints.split_fastq.get(sample=wildcards.sample)
    chunk_dir = chk.output[0]
    chunks = glob_wildcards(
        os.path.join(chunk_dir, f"{wildcards.sample}_R1.{{chunk}}.fq.gz")
    ).chunk
    return sorted(chunks)

def get_chunk_bams(wildcards):
    """Collect all per-chunk tagged BAMs for merging."""
    chunks = _get_chunks_for_sample(wildcards)
    return expand(
        "03.mapping/chunks/{sample}/{sample}.{chunk}_{genome}.bam",
        sample=wildcards.sample, chunk=chunks, genome=wildcards.genome,
    )

def get_chunk_qc_logs(wildcards):
    """Collect all per-chunk QC logs for merging."""
    chunks = _get_chunks_for_sample(wildcards)
    return expand(
        "00.qc/{sample}.{chunk}_qc.log",
        sample=wildcards.sample, chunk=chunks,
    )

###
### Rules
###

wildcard_constraints:
    genome="[^._/]+",
    sample="[^._/]+",
    chunk="part_\\d+"

rule all:
    input:
        expand("03.mapping/{sample}_{genome}.sc.pairs.gz", sample=config["sample_id"], genome=config["genome"]),
        expand("03.mapping/{sample}_{genome}.sc.pairs.gz.px2", sample=config["sample_id"], genome=config["genome"]),
        expand("04.matrices/{sample}_{genome}.mcool", sample=config["sample_id"], genome=config["genome"]),
        expand("03.mapping/{sample}_{genome}.sc.pairdedup.summary.txt", sample=config["sample_id"], genome=config["genome"]),
        expand("03.mapping/{sample}_{genome}.PairCount.stat.csv", sample=config["sample_id"], genome=config["genome"]),
        expand("03.mapping/{sample}_{genome}.sc.pairdedup.txt", sample=config["sample_id"], genome=config["genome"]),
        expand("03.mapping/{sample}_{genome}.bam", sample=config["sample_id"], genome=config["genome"]),
        expand("00.qc/{sample}_qc.log", sample=config["sample_id"])

### ═══════════════════════════════════════════════════════════════════
### Per-chunk processing (steps 0–5)
### ═══════════════════════════════════════════════════════════════════

### Step 0: split FASTQs into chunks
checkpoint split_fastq:
    input:
        R1="01.rawdata/{sample}_R1.fq.gz",
        R2="01.rawdata/{sample}_R2.fq.gz",
        R3="01.rawdata/{sample}_R3.fq.gz"
    output:
        directory("01.rawdata/chunks/{sample}")
    params:
        n_chunks=lambda w: len(CHUNKS[w.sample])
    threads: 4
    run:
        os.makedirs(output[0], exist_ok=True)
        n = params.n_chunks
        if n == 1:
            # Single chunk: symlink instead of splitting
            for read in ["R1", "R2", "R3"]:
                src = os.path.realpath(f"01.rawdata/{wildcards.sample}_{read}.fq.gz")
                dst = os.path.join(output[0], f"{wildcards.sample}_{read}.part_001.fq.gz")
                os.symlink(src, dst)
        else:
            for read in ["R1", "R2", "R3"]:
                fq = f"01.rawdata/{wildcards.sample}_{read}.fq.gz"
                shell(f"seqkit split2 -p {n} -j {{threads}} -O {{output[0]}} -1 {fq}")
            # Rename seqkit output: *.fq.part_NNN.gz -> *.part_NNN.fq.gz
            for f in _glob.glob(os.path.join(output[0], "*.fq.part_*.gz")):
                base = os.path.basename(f)
                new_name = _re.sub(r'\.fq\.part_(\d+)\.gz$', r'.part_\1.fq.gz', base)
                if new_name != base:
                    os.rename(f, os.path.join(output[0], new_name))

### Step 1: align R2 (barcode reads) to barcode index — per chunk
rule align_bc_chunk:
    input:
        R2_fastq="01.rawdata/chunks/{sample}/{sample}_R2.{chunk}.fq.gz"
    output:
        R2_bam=temp("01.rawdata/chunks/{sample}/{sample}_R2.{chunk}_BC.bam")
    threads: 16
    shell:
        """
        bowtie -p {threads} -S {bc_ref} {input.R2_fastq} | \
            samtools view -b -F 4 -@ {threads} -o {output.R2_bam} -
        """

### Step 2: stamp barcodes onto R1/R3 read headers — per chunk
rule stamp_hic_chunk:
    input:
        R1_fastq="01.rawdata/chunks/{sample}/{sample}_R1.{chunk}.fq.gz",
        R3_fastq="01.rawdata/chunks/{sample}/{sample}_R3.{chunk}.fq.gz",
        R2_bam="01.rawdata/chunks/{sample}/{sample}_R2.{chunk}_BC.bam"
    output:
        R1_bc_fastq=temp("01.rawdata/chunks/{sample}/{sample}_R1.{chunk}_BC_cov.fq.gz"),
        R3_bc_fastq=temp("01.rawdata/chunks/{sample}/{sample}_R3.{chunk}_BC_cov.fq.gz"),
        stats="00.qc/{sample}.{chunk}_qc.log"
    shell:
        """
        rupture stamp \
            --bam {input.R2_bam} \
            --r1 {input.R1_fastq} \
            --r3 {input.R3_fastq} \
            --out-r1 {output.R1_bc_fastq} \
            --out-r3 {output.R3_bc_fastq} \
            --bc-tag BC --min-mapq 40 \
            --tmp-dir . \
            2>{output.stats}
        """

### Step 3: quality trimming — per chunk
rule trim_dna_chunk:
    input:
        R1_cov="01.rawdata/chunks/{sample}/{sample}_R1.{chunk}_BC_cov.fq.gz",
        R3_cov="01.rawdata/chunks/{sample}/{sample}_R3.{chunk}_BC_cov.fq.gz"
    output:
        R1_trimmed=temp("02.trimmed/chunks/{sample}/{sample}_R1.{chunk}_BC_cov_val_1.fq.gz"),
        R3_trimmed=temp("02.trimmed/chunks/{sample}/{sample}_R3.{chunk}_BC_cov_val_2.fq.gz"),
        R1_stats=temp("02.trimmed/chunks/{sample}/{sample}_R1.{chunk}_BC_cov.fq.gz_trimming_report.txt"),
        R3_stats=temp("02.trimmed/chunks/{sample}/{sample}_R3.{chunk}_BC_cov.fq.gz_trimming_report.txt")
    threads: 16
    shell:
        "trim_galore --cores {threads} -q 20 --paired {input.R1_cov} {input.R3_cov} -o 02.trimmed/chunks/{wildcards.sample}/"

### Step 4: align to reference genome — per chunk
rule bwa_chunk:
    input:
        R1_trimmed="02.trimmed/chunks/{sample}/{sample}_R1.{chunk}_BC_cov_val_1.fq.gz",
        R3_trimmed="02.trimmed/chunks/{sample}/{sample}_R3.{chunk}_BC_cov_val_2.fq.gz"
    output:
        bam=temp("03.mapping/chunks/{sample}/{sample}.{chunk}_{genome}.raw.bam"),
        stats=temp("03.mapping/chunks/{sample}/{sample}.{chunk}_{genome}.bwa.log")
    threads: 16
    shell:
        "(bwa mem -SP5M -T0 -t{threads} {ref} {input.R1_trimmed} {input.R3_trimmed} | samtools view -bhS - > {output.bam}) 2>{output.stats}"

### Step 5: extract barcode from read name into CB BAM tag — per chunk
rule add_field_chunk:
    input:
        "03.mapping/chunks/{sample}/{sample}.{chunk}_{genome}.raw.bam"
    output:
        temp("03.mapping/chunks/{sample}/{sample}.{chunk}_{genome}.bam")
    shell:
        """
        rupture add-tag --input {input} --output {output} --field -1 --tag CB --sep ":"
        """

### ═══════════════════════════════════════════════════════════════════
### Merge stage
### ═══════════════════════════════════════════════════════════════════

### Step 5b: merge chunk BAMs into one sample BAM
rule merge_chunk_bams:
    input:
        get_chunk_bams
    output:
        "03.mapping/{sample}_{genome}.bam"
    threads: 16
    run:
        if len(input) == 1:
            shell("ln -sf $(realpath {input[0]}) {output[0]}")
        else:
            shell("samtools merge -@ {threads} {output} {input}")

### Merge per-chunk QC logs into one sample QC log
rule merge_qc_logs:
    input:
        get_chunk_qc_logs
    output:
        "00.qc/{sample}_qc.log"
    shell:
        "cat {input} > {output}"

### ═══════════════════════════════════════════════════════════════════
### Single-stream processing (steps 6–13, unchanged)
### ═══════════════════════════════════════════════════════════════════

### Step 6: merge with previous mapping if available, otherwise just link
rule prepare_work_bam:
    input:
        get_merge_inputs
    output:
        temp("03.mapping/{sample}_work_{genome}.bam")
    threads: 16
    run:
        if len(input) > 1:
            shell("samtools merge -@ {threads} {output} {input}")
        else:
            shell("ln -sf $(realpath {input[0]}) {output[0]}")

### Step 7: parse BAM to pairs format
rule pairtools_parse:
    input:
        "03.mapping/{sample}_work_{genome}.bam"
    output:
        pairsam=temp("03.mapping/{sample}_{genome}.pairsam"),
        stats="03.mapping/{sample}_{genome}.pairparse.txt"
    threads: 16
    shell:
        '''
        samtools view -h {input} | \
        pairtools parse --min-mapq 30 --walks-policy all \
            --nproc-in {threads} --nproc-out {threads} \
            --max-inter-align-gap 30 \
            --chroms-path {chrom_size} \
            --assembly {wildcards.genome} \
            --output-stats {output.stats} \
            --add-columns CB \
            -o {output.pairsam}
        '''

### Step 8: sort pairs
rule pairtools_sort:
    input:
        "03.mapping/{sample}_{genome}.pairsam"
    output:
        temp("03.mapping/{sample}_{genome}_sorted.pairsam")
    threads: 16
    shell:
        "pairtools sort --nproc {threads} --memory 16G --tmpdir=03.mapping -o {output} {input}"

### Step 9: deduplicate
rule pairtools_dedup:
    input:
        "03.mapping/{sample}_{genome}_sorted.pairsam"
    output:
        dedup_pairsam=temp("03.mapping/{sample}_{genome}_dedup.pairsam"),
        stats="03.mapping/{sample}_{genome}.sc.pairdedup.txt"
    threads: 16
    shell:
        '''
        pairtools dedup --nproc-in {threads} --nproc-out {threads} \
            --extra-col-pair "CB1" "CB2" \
            --mark-dups \
            -o {output.dedup_pairsam} \
            --output-stats {output.stats} \
            {input}
        '''

### Step 10: split deduped pairsam into pairs + BAM
rule pairtools_split:
    input:
        "03.mapping/{sample}_{genome}_dedup.pairsam"
    output:
        pairs=temp("03.mapping/{sample}_{genome}.sc.pairs"),
        pairbam="03.mapping/{sample}_{genome}.sc.pairtools.bam"
    threads: 16
    shell:
        '''
        pairtools split --nproc-in {threads} --nproc-out {threads} \
            --output-pairs {output.pairs} \
            --output-sam - {input} | \
            samtools view -bS -@ {threads} | \
            samtools sort -T 03.mapping/ -@ {threads} -o {output.pairbam}
        '''

### Step 11: QC statistics
rule QC:
    input:
        stats="03.mapping/{sample}_{genome}.sc.pairdedup.txt",
        pairs="03.mapping/{sample}_{genome}.sc.pairs"
    output:
        summary="03.mapping/{sample}_{genome}.sc.pairdedup.summary.txt",
        count_file="03.mapping/{sample}_{genome}.PairCount.stat.csv"
    params:
        count_prefix=lambda w: f"03.mapping/{w.sample}_{w.genome}.PairCount"
    shell:
        '''
        rupture summarize-qc --input {input.stats} --output {output.summary}
        rupture count-pairs --input {input.pairs} --output {params.count_prefix}
        '''

### Step 12: compress and index pairs
rule bgzip:
    input:
        "03.mapping/{sample}_{genome}.sc.pairs"
    output:
        pairs="03.mapping/{sample}_{genome}.sc.pairs.gz",
        index="03.mapping/{sample}_{genome}.sc.pairs.gz.px2"
    shell:
        '''
        bgzip -c {input} > {output.pairs}
        pairix -f {output.pairs}
        '''

### Step 13: generate multi-resolution contact matrices
rule pair2cool:
    input:
        "03.mapping/{sample}_{genome}.sc.pairs.gz"
    output:
        cool=temp(f"04.matrices/{{sample}}_{{genome}}_{bin_size}.cool"),
        mcool="04.matrices/{sample}_{genome}.mcool"
    threads: 16
    shell:
        '''
        cooler cload pairix {chrom_size}:{bin_size} {input} {output.cool}
        cooler zoomify --balance --balance-args '--convergence-policy store_nan' \
            -p {threads} -o {output.mcool} \
            -r 5000,10000,25000,50000,100000,250000,500000,1000000,2500000 \
            {output.cool}
        '''
